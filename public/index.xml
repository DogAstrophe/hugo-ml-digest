<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>AI Digest</title>
    <link>//localhost:1313/</link>
    <description>Recent content on AI Digest</description>
    <generator>Hugo -- 0.145.0</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 18 Apr 2025 10:22:13 +0000</lastBuildDate>
    <atom:link href="//localhost:1313/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>#ai #artificialintelligence #robotics #shorts #viralreels #chennai #tamil #tamilnadu #foot</title>
      <link>//localhost:1313/video/ai-artificialintelligence-robotics-shorts-viralreels-chennai-tamil-tamilnadu-foot-1744971733105-465713/</link>
      <pubDate>Fri, 18 Apr 2025 10:22:13 +0000</pubDate>
      <guid>//localhost:1313/video/ai-artificialintelligence-robotics-shorts-viralreels-chennai-tamil-tamilnadu-foot-1744971733105-465713/</guid>
      <description>&lt;p&gt;For To Know Few More Details Kindly Call or Visit Direct Consultation Pls&amp;hellip; Halina The Cupping Clinic 523(342/1), Anna Salai &amp;hellip;&lt;/p&gt;</description>
    </item>
    <item>
      <title>#ai #nextgenai #tech #artificialintelligence #html #openai #machinelearning</title>
      <link>//localhost:1313/video/ai-nextgenai-tech-artificialintelligence-html-openai-machinelearning-1744971733104-584685/</link>
      <pubDate>Fri, 18 Apr 2025 10:22:13 +0000</pubDate>
      <guid>//localhost:1313/video/ai-nextgenai-tech-artificialintelligence-html-openai-machinelearning-1744971733104-584685/</guid>
      <description></description>
    </item>
    <item>
      <title>#aitutorials #artificialintelligence #ai #techhalla #generativeai</title>
      <link>//localhost:1313/video/aitutorials-artificialintelligence-ai-techhalla-generativeai-1744971733105-194423/</link>
      <pubDate>Fri, 18 Apr 2025 10:22:13 +0000</pubDate>
      <guid>//localhost:1313/video/aitutorials-artificialintelligence-ai-techhalla-generativeai-1744971733105-194423/</guid>
      <description></description>
    </item>
    <item>
      <title>#animals #pubgmobile #snake #artificialintelligence #ai #trending #rescue #horse #shortvideo</title>
      <link>//localhost:1313/video/animals-pubgmobile-snake-artificialintelligence-ai-trending-rescue-horse-shortvideo-1744971733104-930606/</link>
      <pubDate>Fri, 18 Apr 2025 10:22:13 +0000</pubDate>
      <guid>//localhost:1313/video/animals-pubgmobile-snake-artificialintelligence-ai-trending-rescue-horse-shortvideo-1744971733104-930606/</guid>
      <description></description>
    </item>
    <item>
      <title>#cat #artificialintelligence #cute #aiart #funny</title>
      <link>//localhost:1313/video/cat-artificialintelligence-cute-aiart-funny-1744971733105-115100/</link>
      <pubDate>Fri, 18 Apr 2025 10:22:13 +0000</pubDate>
      <guid>//localhost:1313/video/cat-artificialintelligence-cute-aiart-funny-1744971733105-115100/</guid>
      <description></description>
    </item>
    <item>
      <title>#Dhoni#Robo Dog#Artificial Intelligence#AI#Technology#ROBOTECH</title>
      <link>//localhost:1313/video/dhoni-robo-dog-artificial-intelligence-ai-technology-robotech-1744971733105-722981/</link>
      <pubDate>Fri, 18 Apr 2025 10:22:13 +0000</pubDate>
      <guid>//localhost:1313/video/dhoni-robo-dog-artificial-intelligence-ai-technology-robotech-1744971733105-722981/</guid>
      <description></description>
    </item>
    <item>
      <title>$	exttt{Complex-Edit}$: CoT-Like Instruction Generation for Complexity-Controllable Image Editing Benchmark</title>
      <link>//localhost:1313/paper/texttt-complex-edit-cot-like-instruction-generation-for-complexity-controllable-image-editing-benchmark-1744971733104-183310/</link>
      <pubDate>Fri, 18 Apr 2025 10:22:13 +0000</pubDate>
      <guid>//localhost:1313/paper/texttt-complex-edit-cot-like-instruction-generation-for-complexity-controllable-image-editing-benchmark-1744971733104-183310/</guid>
      <description>&lt;p&gt;We introduce $\texttt{Complex-Edit}$, a comprehensive benchmark designed to
systematically evaluate instruction-based image editing models across
instructions of varying complexity. To develop this benchmark, we harness
GPT-4o to automatically collect a diverse set of editing instructions at scale.
Our approach follows a well-structured &lt;code&gt;Chain-of-Edit&#39;&#39; pipeline: we first generate individual atomic editing tasks independently and then integrate them to form cohesive, complex instructions. Additionally, we introduce a suite of metrics to assess various aspects of editing performance, along with a VLM-based auto-evaluation pipeline that supports large-scale assessments. Our benchmark yields several notable insights: 1) Open-source models significantly underperform relative to proprietary, closed-source models, with the performance gap widening as instruction complexity increases; 2) Increased instructional complexity primarily impairs the models&#39; ability to retain key elements from the input images and to preserve the overall aesthetic quality; 3) Decomposing a complex instruction into a sequence of atomic steps, executed in a step-by-step manner, substantially degrades performance across multiple metrics; 4) A straightforward Best-of-N selection strategy improves results for both direct editing and the step-by-step sequential approach; and 5) We observe a &lt;/code&gt;curse of synthetic data&amp;rsquo;&amp;rsquo;: when synthetic data is involved in model
training, the edited images from such models tend to appear increasingly
synthetic as the complexity of the editing instructions rises &amp;ndash; a phenomenon
that intriguingly also manifests in the latest GPT-4o outputs.&lt;/p&gt;</description>
    </item>
    <item>
      <title>$5000 AI Course for FREE | App Development | #technology #artificialintelligence #education #AI</title>
      <link>//localhost:1313/video/5000-ai-course-for-free-app-development-technology-artificialintelligence-education-ai-1744971733105-499010/</link>
      <pubDate>Fri, 18 Apr 2025 10:22:13 +0000</pubDate>
      <guid>//localhost:1313/video/5000-ai-course-for-free-app-development-technology-artificialintelligence-education-ai-1744971733105-499010/</guid>
      <description>&lt;p&gt;ka ye AI Course meh tumhe FREE me de raha hu! Aur tum is se seekh ke mahine ke 4 se 5 lakh tak kama sakte ho! Reel save &amp;hellip;&lt;/p&gt;</description>
    </item>
    <item>
      <title>A general language model for peptide identification</title>
      <link>//localhost:1313/paper/a-general-language-model-for-peptide-identification-1744971733104-294521/</link>
      <pubDate>Fri, 18 Apr 2025 10:22:13 +0000</pubDate>
      <guid>//localhost:1313/paper/a-general-language-model-for-peptide-identification-1744971733104-294521/</guid>
      <description>&lt;p&gt;Advances in peptide identification are revolutionizing our ability to
decipher protein functions and accelerate therapeutic discovery. We present
PDeepPP, a deep learning framework that integrates pretrained protein language
models with parallel transformer-CNN architectures, achieving state-of-the-art
performance in peptide characterization tasks. The model&amp;rsquo;s hybrid architecture
demonstrates unique capabilities in capturing both local sequence motifs and
global structural features, as evidenced by 29% improved cluster separation in
UMAP visualizations compared to conventional approaches. Evaluated across 33
biological recognition tasks - including post-translational modification site
prediction and bioactive peptide identification - PDeepPP outperformed existing
methods in 25 tasks with average AUC improvements of 4.2%. Notably, it achieved
0.9726 accuracy with PR AUC 0.9977 in antimicrobial peptide detection while
reducing false negatives by 37.5% in antimalarial recognition scenarios. This
framework enables accurate large-scale peptide analysis, achieving 218*
acceleration over sequence-alignment-based methods while maintaining 99.5%
specificity in critical glycosylation site detection.PDeepPP establishes a new
paradigm for computational peptide analysis through its synergistic
architecture design, enabling rapid yet precise functional annotation that
bridges molecular pattern recognition with translational biomedical
applications.We have made our implementation, including code, data, and
pretrained models, publicly available via GitHub
(&lt;a href=&#34;https://github.com/fondress/PDeepPP&#34;&gt;https://github.com/fondress/PDeepPP&lt;/a&gt;) and Hugging Face
(&lt;a href=&#34;https://huggingface.co/fondress/PDeppPP)&#34;&gt;https://huggingface.co/fondress/PDeppPP)&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>A New Semidefinite Relaxation for Linear and Piecewise-Affine Optimal Control with Time Scaling</title>
      <link>//localhost:1313/paper/a-new-semidefinite-relaxation-for-linear-and-piecewise-affine-optimal-control-with-time-scaling-1744971733104-423009/</link>
      <pubDate>Fri, 18 Apr 2025 10:22:13 +0000</pubDate>
      <guid>//localhost:1313/paper/a-new-semidefinite-relaxation-for-linear-and-piecewise-affine-optimal-control-with-time-scaling-1744971733104-423009/</guid>
      <description>&lt;p&gt;We introduce a semidefinite relaxation for optimal control of linear systems
with time scaling. These problems are inherently nonconvex, since the system
dynamics involves bilinear products between the discretization time step and
the system state and controls. The proposed relaxation is closely related to
the standard second-order semidefinite relaxation for quadratic constraints,
but we carefully select a subset of the possible bilinear terms and apply a
change of variables to achieve empirically tight relaxations while keeping the
computational load light. We further extend our method to handle
piecewise-affine (PWA) systems by formulating the PWA optimal-control problem
as a shortest-path problem in a graph of convex sets (GCS). In this GCS,
different paths represent different mode sequences for the PWA system, and the
convex sets model the relaxed dynamics within each mode. By combining a tight
convex relaxation of the GCS problem with our semidefinite relaxation with time
scaling, we can solve PWA optimal-control problems through a single
semidefinite program.&lt;/p&gt;</description>
    </item>
    <item>
      <title>AerialMegaDepth: Learning Aerial-Ground Reconstruction and View Synthesis</title>
      <link>//localhost:1313/paper/aerialmegadepth-learning-aerial-ground-reconstruction-and-view-synthesis-1744971733104-228844/</link>
      <pubDate>Fri, 18 Apr 2025 10:22:13 +0000</pubDate>
      <guid>//localhost:1313/paper/aerialmegadepth-learning-aerial-ground-reconstruction-and-view-synthesis-1744971733104-228844/</guid>
      <description>&lt;p&gt;We explore the task of geometric reconstruction of images captured from a
mixture of ground and aerial views. Current state-of-the-art learning-based
approaches fail to handle the extreme viewpoint variation between aerial-ground
image pairs. Our hypothesis is that the lack of high-quality, co-registered
aerial-ground datasets for training is a key reason for this failure. Such data
is difficult to assemble precisely because it is difficult to reconstruct in a
scalable way. To overcome this challenge, we propose a scalable framework
combining pseudo-synthetic renderings from 3D city-wide meshes (e.g., Google
Earth) with real, ground-level crowd-sourced images (e.g., MegaDepth). The
pseudo-synthetic data simulates a wide range of aerial viewpoints, while the
real, crowd-sourced images help improve visual fidelity for ground-level images
where mesh-based renderings lack sufficient detail, effectively bridging the
domain gap between real images and pseudo-synthetic renderings. Using this
hybrid dataset, we fine-tune several state-of-the-art algorithms and achieve
significant improvements on real-world, zero-shot aerial-ground tasks. For
example, we observe that baseline DUSt3R localizes fewer than 5% of
aerial-ground pairs within 5 degrees of camera rotation error, while
fine-tuning with our data raises accuracy to nearly 56%, addressing a major
failure point in handling large viewpoint changes. Beyond camera estimation and
scene reconstruction, our dataset also improves performance on downstream tasks
like novel-view synthesis in challenging aerial-ground scenarios, demonstrating
the practical value of our approach in real-world applications.&lt;/p&gt;</description>
    </item>
    <item>
      <title>AI yang lagi tren dan harus kamu share! üìà#ai #trendingai #artificialintelligence #contentcreator</title>
      <link>//localhost:1313/video/ai-yang-lagi-tren-dan-harus-kamu-share-ai-trendingai-artificialintelligence-contentcreator-1744971733105-704796/</link>
      <pubDate>Fri, 18 Apr 2025 10:22:13 +0000</pubDate>
      <guid>//localhost:1313/video/ai-yang-lagi-tren-dan-harus-kamu-share-ai-trendingai-artificialintelligence-contentcreator-1744971733105-704796/</guid>
      <description></description>
    </item>
    <item>
      <title>AI ‡§∏‡•á ‡§°‡§∞‡§®‡•á ‡§ï‡•Ä 3 ‡§∏‡§¨‡§∏‡•á ‡§¨‡§°‡§º‡•Ä ‡§µ‡§ú‡§π ?        #viralshort #artificialintelligence #amzingfacts #viralfacts</title>
      <link>//localhost:1313/video/ai-3-viralshort-artificialintelligence-amzingfacts-viralfacts-1744971733105-520661/</link>
      <pubDate>Fri, 18 Apr 2025 10:22:13 +0000</pubDate>
      <guid>//localhost:1313/video/ai-3-viralshort-artificialintelligence-amzingfacts-viralfacts-1744971733105-520661/</guid>
      <description>&lt;p&gt;AI ‡§∏‡•á ‡§°‡§∞‡§®‡•á ‡§ï‡•Ä 3 ‡§∏‡§¨‡§∏‡•á ‡§¨‡§°‡§º‡•Ä ‡§µ‡§ú‡§π ? #viralshort #artificialintelligence #amzingfacts #viralfacts.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Aligning Constraint Generation with Design Intent in Parametric CAD</title>
      <link>//localhost:1313/paper/aligning-constraint-generation-with-design-intent-in-parametric-cad-1744971733104-976365/</link>
      <pubDate>Fri, 18 Apr 2025 10:22:13 +0000</pubDate>
      <guid>//localhost:1313/paper/aligning-constraint-generation-with-design-intent-in-parametric-cad-1744971733104-976365/</guid>
      <description>&lt;p&gt;We adapt alignment techniques from reasoning LLMs to the task of generating
engineering sketch constraints found in computer-aided design (CAD) models.
Engineering sketches consist of geometric primitives (e.g. points, lines)
connected by constraints (e.g. perpendicular, tangent) that define the
relationships between them. For a design to be easily editable, the constraints
must effectively capture design intent, ensuring the geometry updates
predictably when parameters change. Although current approaches can generate
CAD designs, an open challenge remains to align model outputs with design
intent, we label this problem `design alignment&amp;rsquo;. A critical first step towards
aligning generative CAD models is to generate constraints which fully-constrain
all geometric primitives, without over-constraining or distorting sketch
geometry. Using alignment techniques to train an existing constraint generation
model with feedback from a constraint solver, we are able to fully-constrain
93% of sketches compared to 34% when using a na\&amp;ldquo;ive supervised fine-tuning
(SFT) baseline and only 8.9% without alignment. Our approach can be applied to
any existing constraint generation model and sets the stage for further
research bridging alignment strategies between the language and design domains.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Antidistillation Sampling</title>
      <link>//localhost:1313/paper/antidistillation-sampling-1744971733104-23698/</link>
      <pubDate>Fri, 18 Apr 2025 10:22:13 +0000</pubDate>
      <guid>//localhost:1313/paper/antidistillation-sampling-1744971733104-23698/</guid>
      <description>&lt;p&gt;Frontier models that generate extended reasoning traces inadvertently produce
rich token sequences that can facilitate model distillation. Recognizing this
vulnerability, model owners may seek sampling strategies that limit the
effectiveness of distillation without compromising model performance.
\emph{Antidistillation sampling} provides exactly this capability. By
strategically modifying a model&amp;rsquo;s next-token probability distribution,
antidistillation sampling poisons reasoning traces, rendering them
significantly less effective for distillation while preserving the model&amp;rsquo;s
practical utility. For further details, see &lt;a href=&#34;https://antidistillation.com&#34;&gt;https://antidistillation.com&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Artificial Intelligence #shorts #science #ai #brainripe #artificialintelligence</title>
      <link>//localhost:1313/video/artificial-intelligence-shorts-science-ai-brainripe-artificialintelligence-1744971733105-578988/</link>
      <pubDate>Fri, 18 Apr 2025 10:22:13 +0000</pubDate>
      <guid>//localhost:1313/video/artificial-intelligence-shorts-science-ai-brainripe-artificialintelligence-1744971733105-578988/</guid>
      <description>&lt;p&gt;AI isn&amp;rsquo;t just a tool‚Äîit&amp;rsquo;s about to become invisible like electricity. Every job, every field, every moment of your day will be &amp;hellip;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Artificial intelligence animal hybrid fusion. #animalfusion #hybrids #animals #fusion #shorts</title>
      <link>//localhost:1313/video/artificial-intelligence-animal-hybrid-fusion-animalfusion-hybrids-animals-fusion-shorts-1744971733104-110711/</link>
      <pubDate>Fri, 18 Apr 2025 10:22:13 +0000</pubDate>
      <guid>//localhost:1313/video/artificial-intelligence-animal-hybrid-fusion-animalfusion-hybrids-animals-fusion-shorts-1744971733104-110711/</guid>
      <description></description>
    </item>
    <item>
      <title>ARTIFICIAL INTELLIGENCE ARTISTüìç#ai #aianimal #new #viralvideos</title>
      <link>//localhost:1313/video/artificial-intelligence-artist-ai-aianimal-new-viralvideos-1744971733104-697229/</link>
      <pubDate>Fri, 18 Apr 2025 10:22:13 +0000</pubDate>
      <guid>//localhost:1313/video/artificial-intelligence-artist-ai-aianimal-new-viralvideos-1744971733104-697229/</guid>
      <description>&lt;p&gt;ARTIFICIAL INTELLIGENCE ARTIST   #ai #aianimal #new.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Artificial Intelligence Full Course 2025 | Artificial Intelligence Tutorial | AI Course |Simplilearn</title>
      <link>//localhost:1313/video/artificial-intelligence-full-course-2025-artificial-intelligence-tutorial-ai-course-simplilearn-1744971733105-943435/</link>
      <pubDate>Fri, 18 Apr 2025 10:22:13 +0000</pubDate>
      <guid>//localhost:1313/video/artificial-intelligence-full-course-2025-artificial-intelligence-tutorial-ai-course-simplilearn-1744971733105-943435/</guid>
      <description>&lt;p&gt;Artificial Intelligence Engineer (IBM) &amp;hellip;&lt;/p&gt;</description>
    </item>
    <item>
      <title>artificial intelligence human animal hybrid fusion. #ai #aiart #animalfusion #hybrids #hibrido</title>
      <link>//localhost:1313/video/artificial-intelligence-human-animal-hybrid-fusion-ai-aiart-animalfusion-hybrids-hibrido-1744971733104-614808/</link>
      <pubDate>Fri, 18 Apr 2025 10:22:13 +0000</pubDate>
      <guid>//localhost:1313/video/artificial-intelligence-human-animal-hybrid-fusion-ai-aiart-animalfusion-hybrids-hibrido-1744971733104-614808/</guid>
      <description></description>
    </item>
    <item>
      <title>Avoiding Shuffles and Skewed Data #ai #artificialintelligence #machinelearning #aiagent #Avoiding</title>
      <link>//localhost:1313/video/avoiding-shuffles-and-skewed-data-ai-artificialintelligence-machinelearning-aiagent-avoiding-1744971733105-780870/</link>
      <pubDate>Fri, 18 Apr 2025 10:22:13 +0000</pubDate>
      <guid>//localhost:1313/video/avoiding-shuffles-and-skewed-data-ai-artificialintelligence-machinelearning-aiagent-avoiding-1744971733105-780870/</guid>
      <description>&lt;p&gt;genaiexp Data shuffling is a costly operation in Spark, involving data transfer across the network, which can significantly slow &amp;hellip;&lt;/p&gt;</description>
    </item>
    <item>
      <title>BABIES Everywhere Are Getting FREE Artificial Intelligence Bodyguards Part 2 #babiesworld</title>
      <link>//localhost:1313/video/babies-everywhere-are-getting-free-artificial-intelligence-bodyguards-part-2-babiesworld-1744971733105-167563/</link>
      <pubDate>Fri, 18 Apr 2025 10:22:13 +0000</pubDate>
      <guid>//localhost:1313/video/babies-everywhere-are-getting-free-artificial-intelligence-bodyguards-part-2-babiesworld-1744971733105-167563/</guid>
      <description>&lt;p&gt;Ai Transformation Video From Various Items Into A Cool Robot That Has A Luxurious Appearance And Looks Very Advanced!&lt;/p&gt;</description>
    </item>
    <item>
      <title>Deep information about artificial intelligence. Discover and develop your skills.#explore</title>
      <link>//localhost:1313/video/deep-information-about-artificial-intelligence-discover-and-develop-your-skills-explore-1744971733104-132020/</link>
      <pubDate>Fri, 18 Apr 2025 10:22:13 +0000</pubDate>
      <guid>//localhost:1313/video/deep-information-about-artificial-intelligence-discover-and-develop-your-skills-explore-1744971733104-132020/</guid>
      <description></description>
    </item>
    <item>
      <title>Differentially Private Sequential Learning</title>
      <link>//localhost:1313/paper/differentially-private-sequential-learning-1744971733104-230529/</link>
      <pubDate>Fri, 18 Apr 2025 10:22:13 +0000</pubDate>
      <guid>//localhost:1313/paper/differentially-private-sequential-learning-1744971733104-230529/</guid>
      <description>&lt;p&gt;In a differentially private sequential learning setting, agents introduce
endogenous noise into their actions to maintain privacy. Applying this to a
standard sequential learning model leads to different outcomes for continuous
vs. binary signals. For continuous signals with a nonzero privacy budget, we
introduce a novel smoothed randomized response mechanism that adapts noise
based on distance to a threshold, unlike traditional randomized response, which
applies uniform noise. This enables agents&amp;rsquo; actions to better reflect both
private signals and observed history, accelerating asymptotic learning speed to
$\Theta_{\epsilon}(\log(n))$, compared to $\Theta(\sqrt{\log(n)})$ in the
non-private regime where privacy budget is infinite. Moreover, in the
non-private setting, the expected stopping time for the first correct decision
and the number of incorrect actions diverge, meaning early agents may make
mistakes for an unreasonably long period. In contrast, under a finite privacy
budget $\epsilon \in (0,1)$, both remain finite, highlighting a stark contrast
between private and non-private learning. Learning with continuous signals in
the private regime is more efficient, as smooth randomized response enhances
the log-likelihood ratio over time, improving information aggregation.
Conversely, for binary signals, differential privacy noise hinders learning, as
agents tend to use a constant randomized response strategy before an
information cascade forms, reducing action informativeness and hampering the
overall process.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Digital Twin Generation from Visual Data: A Survey</title>
      <link>//localhost:1313/paper/digital-twin-generation-from-visual-data-a-survey-1744971733104-711403/</link>
      <pubDate>Fri, 18 Apr 2025 10:22:13 +0000</pubDate>
      <guid>//localhost:1313/paper/digital-twin-generation-from-visual-data-a-survey-1744971733104-711403/</guid>
      <description>&lt;p&gt;This survey explores recent developments in generating digital twins from
videos. Such digital twins can be used for robotics application, media content
creation, or design and construction works. We analyze various approaches,
including 3D Gaussian Splatting, generative in-painting, semantic segmentation,
and foundation models highlighting their advantages and limitations.
Additionally, we discuss challenges such as occlusions, lighting variations,
and scalability, as well as potential future research directions. This survey
aims to provide a comprehensive overview of state-of-the-art methodologies and
their implications for real-world applications. Awesome list:
&lt;a href=&#34;https://github.com/ndrwmlnk/awesome-digital-twins&#34;&gt;https://github.com/ndrwmlnk/awesome-digital-twins&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Exploring Expert Failures Improves LLM Agent Tuning</title>
      <link>//localhost:1313/paper/exploring-expert-failures-improves-llm-agent-tuning-1744971733104-92638/</link>
      <pubDate>Fri, 18 Apr 2025 10:22:13 +0000</pubDate>
      <guid>//localhost:1313/paper/exploring-expert-failures-improves-llm-agent-tuning-1744971733104-92638/</guid>
      <description>&lt;p&gt;Large Language Models (LLMs) have shown tremendous potential as agents,
excelling at tasks that require multiple rounds of reasoning and interactions.
Rejection Sampling Fine-Tuning (RFT) has emerged as an effective method for
finetuning LLMs as agents: it first imitates expert-generated successful
trajectories and further improves agentic skills through iterative fine-tuning
on successful, self-generated trajectories. However, since the expert (e.g.,
GPT-4) succeeds primarily on simpler subtasks and RFT inherently favors simpler
scenarios, many complex subtasks remain unsolved and persistently
out-of-distribution (OOD). Upon investigating these challenging subtasks, we
discovered that previously failed expert trajectories can often provide
valuable guidance, e.g., plans and key actions, that can significantly improve
agent exploration efficiency and acquisition of critical skills. Motivated by
these observations, we propose Exploring Expert Failures (EEF), which
identifies beneficial actions from failed expert trajectories and integrates
them into the training dataset. Potentially harmful actions are meticulously
excluded to prevent contamination of the model learning process. By leveraging
the beneficial actions in expert failures, EEF successfully solves some
previously unsolvable subtasks and improves agent tuning performance.
Remarkably, our approach achieved a 62% win rate in WebShop, outperforming RFT
(53. 6%) and GPT-4 (35. 6%), and to the best of our knowledge, setting a new
state-of-the-art as the first method to surpass a score of 0.81 in WebShop and
exceed 81 in SciWorld.&lt;/p&gt;</description>
    </item>
    <item>
      <title>fact about artificial intelligence chat GP and Sora</title>
      <link>//localhost:1313/video/fact-about-artificial-intelligence-chat-gp-and-sora-1744971733104-723758/</link>
      <pubDate>Fri, 18 Apr 2025 10:22:13 +0000</pubDate>
      <guid>//localhost:1313/video/fact-about-artificial-intelligence-chat-gp-and-sora-1744971733104-723758/</guid>
      <description></description>
    </item>
    <item>
      <title>Here&amp;#39;s how artificial intelligence is helping to reunite lost pets</title>
      <link>//localhost:1313/video/here-39-s-how-artificial-intelligence-is-helping-to-reunite-lost-pets-1744971733105-386327/</link>
      <pubDate>Fri, 18 Apr 2025 10:22:13 +0000</pubDate>
      <guid>//localhost:1313/video/here-39-s-how-artificial-intelligence-is-helping-to-reunite-lost-pets-1744971733105-386327/</guid>
      <description>&lt;p&gt;If you have pets, then you know the fear of your pet running off. Now, there&amp;rsquo;s new artificial intelligence tech aiming to make that &amp;hellip;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Imagine a future where artificial intelligence not only understands the intricacies of human</title>
      <link>//localhost:1313/video/imagine-a-future-where-artificial-intelligence-not-only-understands-the-intricacies-of-human-1744971733105-635974/</link>
      <pubDate>Fri, 18 Apr 2025 10:22:13 +0000</pubDate>
      <guid>//localhost:1313/video/imagine-a-future-where-artificial-intelligence-not-only-understands-the-intricacies-of-human-1744971733105-635974/</guid>
      <description>&lt;p&gt;Imagine a future where artificial intelligence not only understands the intricacies of human cognition but also unlocks &amp;hellip;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Inteligencia artificial #maltesedog #pets  #ai #artificial intelligence</title>
      <link>//localhost:1313/video/inteligencia-artificial-maltesedog-pets-ai-artificial-intelligence-1744971733105-437733/</link>
      <pubDate>Fri, 18 Apr 2025 10:22:13 +0000</pubDate>
      <guid>//localhost:1313/video/inteligencia-artificial-maltesedog-pets-ai-artificial-intelligence-1744971733105-437733/</guid>
      <description></description>
    </item>
    <item>
      <title>INTELIGENCIA ARTIFICIAL NATIVAMENTE NO GOOGLE PLANILHAS - COMO USAR?</title>
      <link>//localhost:1313/video/inteligencia-artificial-nativamente-no-google-planilhas-como-usar-1744971733105-403822/</link>
      <pubDate>Fri, 18 Apr 2025 10:22:13 +0000</pubDate>
      <guid>//localhost:1313/video/inteligencia-artificial-nativamente-no-google-planilhas-como-usar-1744971733105-403822/</guid>
      <description>&lt;p&gt;Meu curso completo de Google Planilhas: &lt;a href=&#34;https://curso.programacaoemexcel.com.br/?origem=youtube&#34;&gt;https://curso.programacaoemexcel.com.br/?origem=youtube&lt;/a&gt; Centenas de Planilhas &amp;hellip;&lt;/p&gt;</description>
    </item>
    <item>
      <title>It&#39;s All Connected: A Journey Through Test-Time Memorization, Attentional Bias, Retention, and Online Optimization</title>
      <link>//localhost:1313/paper/it-s-all-connected-a-journey-through-test-time-memorization-attentional-bias-retention-and-online-optimization-1744971733104-697057/</link>
      <pubDate>Fri, 18 Apr 2025 10:22:13 +0000</pubDate>
      <guid>//localhost:1313/paper/it-s-all-connected-a-journey-through-test-time-memorization-attentional-bias-retention-and-online-optimization-1744971733104-697057/</guid>
      <description>&lt;p&gt;Designing efficient and effective architectural backbones has been in the
core of research efforts to enhance the capability of foundation models.
Inspired by the human cognitive phenomenon of attentional bias-the natural
tendency to prioritize certain events or stimuli-we reconceptualize neural
architectures, including Transformers, Titans, and modern linear recurrent
neural networks as associative memory modules that learn a mapping of keys and
values using an internal objective, referred to as attentional bias.
Surprisingly, we observed that most existing sequence models leverage either
(1) dot-product similarity, or (2) L2 regression objectives as their
attentional bias. Going beyond these objectives, we present a set of
alternative attentional bias configurations along with their effective
approximations to stabilize their training procedure. We then reinterpret
forgetting mechanisms in modern deep learning architectures as a form of
retention regularization, providing a novel set of forget gates for sequence
models. Building upon these insights, we present Miras, a general framework to
design deep learning architectures based on four choices of: (i) associative
memory architecture, (ii) attentional bias objective, (iii) retention gate, and
(iv) memory learning algorithm. We present three novel sequence models-Moneta,
Yaad, and Memora-that go beyond the power of existing linear RNNs while
maintaining a fast parallelizable training process. Our experiments show
different design choices in Miras yield models with varying strengths. For
example, certain instances of Miras achieve exceptional performance in special
tasks such as language modeling, commonsense reasoning, and recall intensive
tasks, even outperforming Transformers and other modern linear recurrent
models.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Long Range Navigator (LRN): Extending robot planning horizons beyond metric maps</title>
      <link>//localhost:1313/paper/long-range-navigator-lrn-extending-robot-planning-horizons-beyond-metric-maps-1744971733104-796104/</link>
      <pubDate>Fri, 18 Apr 2025 10:22:13 +0000</pubDate>
      <guid>//localhost:1313/paper/long-range-navigator-lrn-extending-robot-planning-horizons-beyond-metric-maps-1744971733104-796104/</guid>
      <description>&lt;p&gt;A robot navigating an outdoor environment with no prior knowledge of the
space must rely on its local sensing to perceive its surroundings and plan.
This can come in the form of a local metric map or local policy with some fixed
horizon. Beyond that, there is a fog of unknown space marked with some fixed
cost. A limited planning horizon can often result in myopic decisions leading
the robot off course or worse, into very difficult terrain. Ideally, we would
like the robot to have full knowledge that can be orders of magnitude larger
than a local cost map. In practice, this is intractable due to sparse sensing
information and often computationally expensive. In this work, we make a key
observation that long-range navigation only necessitates identifying good
frontier directions for planning instead of full map knowledge. To this end, we
propose Long Range Navigator (LRN), that learns an intermediate affordance
representation mapping high-dimensional camera images to `affordable&amp;rsquo; frontiers
for planning, and then optimizing for maximum alignment with the desired goal.
LRN notably is trained entirely on unlabeled ego-centric videos making it easy
to scale and adapt to new platforms. Through extensive off-road experiments on
Spot and a Big Vehicle, we find that augmenting existing navigation stacks with
LRN reduces human interventions at test-time and leads to faster decision
making indicating the relevance of LRN. &lt;a href=&#34;https://personalrobotics.github.io/lrn&#34;&gt;https://personalrobotics.github.io/lrn&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>MalMixer: Few-Shot Malware Classification with Retrieval-Augmented Semi-Supervised Learning</title>
      <link>//localhost:1313/paper/malmixer-few-shot-malware-classification-with-retrieval-augmented-semi-supervised-learning-1744971733104-894614/</link>
      <pubDate>Fri, 18 Apr 2025 10:22:13 +0000</pubDate>
      <guid>//localhost:1313/paper/malmixer-few-shot-malware-classification-with-retrieval-augmented-semi-supervised-learning-1744971733104-894614/</guid>
      <description>&lt;p&gt;Recent growth and proliferation of malware have tested practitioners ability
to promptly classify new samples according to malware families. In contrast to
labor-intensive reverse engineering efforts, machine learning approaches have
demonstrated increased speed and accuracy. However, most existing deep-learning
malware family classifiers must be calibrated using a large number of samples
that are painstakingly manually analyzed before training. Furthermore, as novel
malware samples arise that are beyond the scope of the training set, additional
reverse engineering effort must be employed to update the training set. The
sheer volume of new samples found in the wild creates substantial pressure on
practitioners ability to reverse engineer enough malware to adequately train
modern classifiers. In this paper, we present MalMixer, a malware family
classifier using semi-supervised learning that achieves high accuracy with
sparse training data. We present a domain-knowledge-aware data augmentation
technique for malware feature representations, enhancing few-shot performance
of semi-supervised malware family classification. We show that MalMixer
achieves state-of-the-art performance in few-shot malware family classification
settings. Our research confirms the feasibility and effectiveness of
lightweight, domain-knowledge-aware data augmentation methods for malware
features and shows the capabilities of similar semi-supervised classifiers in
addressing malware classification issues.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Mercedes dualtone maybach s680 v12  #ai #artificialintelligence #shortsfeed #shorts</title>
      <link>//localhost:1313/video/mercedes-dualtone-maybach-s680-v12-ai-artificialintelligence-shortsfeed-shorts-1744971733105-361502/</link>
      <pubDate>Fri, 18 Apr 2025 10:22:13 +0000</pubDate>
      <guid>//localhost:1313/video/mercedes-dualtone-maybach-s680-v12-ai-artificialintelligence-shortsfeed-shorts-1744971733105-361502/</guid>
      <description></description>
    </item>
    <item>
      <title>MIB: A Mechanistic Interpretability Benchmark</title>
      <link>//localhost:1313/paper/mib-a-mechanistic-interpretability-benchmark-1744971733104-336599/</link>
      <pubDate>Fri, 18 Apr 2025 10:22:13 +0000</pubDate>
      <guid>//localhost:1313/paper/mib-a-mechanistic-interpretability-benchmark-1744971733104-336599/</guid>
      <description>&lt;p&gt;How can we know whether new mechanistic interpretability methods achieve real
improvements? In pursuit of meaningful and lasting evaluation standards, we
propose MIB, a benchmark with two tracks spanning four tasks and five models.
MIB favors methods that precisely and concisely recover relevant causal
pathways or specific causal variables in neural language models. The circuit
localization track compares methods that locate the model components - and
connections between them - most important for performing a task (e.g.,
attribution patching or information flow routes). The causal variable
localization track compares methods that featurize a hidden vector, e.g.,
sparse autoencoders (SAEs) or distributed alignment search (DAS), and locate
model features for a causal variable relevant to the task. Using MIB, we find
that attribution and mask optimization methods perform best on circuit
localization. For causal variable localization, we find that the supervised DAS
method performs best, while SAE features are not better than neurons, i.e.,
standard dimensions of hidden vectors. These findings illustrate that MIB
enables meaningful comparisons of methods, and increases our confidence that
there has been real progress in the field.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Novel Demonstration Generation with Gaussian Splatting Enables Robust One-Shot Manipulation</title>
      <link>//localhost:1313/paper/novel-demonstration-generation-with-gaussian-splatting-enables-robust-one-shot-manipulation-1744971733104-790875/</link>
      <pubDate>Fri, 18 Apr 2025 10:22:13 +0000</pubDate>
      <guid>//localhost:1313/paper/novel-demonstration-generation-with-gaussian-splatting-enables-robust-one-shot-manipulation-1744971733104-790875/</guid>
      <description>&lt;p&gt;Visuomotor policies learned from teleoperated demonstrations face challenges
such as lengthy data collection, high costs, and limited data diversity.
Existing approaches address these issues by augmenting image observations in
RGB space or employing Real-to-Sim-to-Real pipelines based on physical
simulators. However, the former is constrained to 2D data augmentation, while
the latter suffers from imprecise physical simulation caused by inaccurate
geometric reconstruction. This paper introduces RoboSplat, a novel method that
generates diverse, visually realistic demonstrations by directly manipulating
3D Gaussians. Specifically, we reconstruct the scene through 3D Gaussian
Splatting (3DGS), directly edit the reconstructed scene, and augment data
across six types of generalization with five techniques: 3D Gaussian
replacement for varying object types, scene appearance, and robot embodiments;
equivariant transformations for different object poses; visual attribute
editing for various lighting conditions; novel view synthesis for new camera
perspectives; and 3D content generation for diverse object types. Comprehensive
real-world experiments demonstrate that RoboSplat significantly enhances the
generalization of visuomotor policies under diverse disturbances. Notably,
while policies trained on hundreds of real-world demonstrations with additional
2D data augmentation achieve an average success rate of 57.2%, RoboSplat
attains 87.8% in one-shot settings across six types of generalization in the
real world.&lt;/p&gt;</description>
    </item>
    <item>
      <title>ODHSR: Online Dense 3D Reconstruction of Humans and Scenes from Monocular Videos</title>
      <link>//localhost:1313/paper/odhsr-online-dense-3d-reconstruction-of-humans-and-scenes-from-monocular-videos-1744971733104-947228/</link>
      <pubDate>Fri, 18 Apr 2025 10:22:13 +0000</pubDate>
      <guid>//localhost:1313/paper/odhsr-online-dense-3d-reconstruction-of-humans-and-scenes-from-monocular-videos-1744971733104-947228/</guid>
      <description>&lt;p&gt;Creating a photorealistic scene and human reconstruction from a single
monocular in-the-wild video figures prominently in the perception of a
human-centric 3D world. Recent neural rendering advances have enabled holistic
human-scene reconstruction but require pre-calibrated camera and human poses,
and days of training time. In this work, we introduce a novel unified framework
that simultaneously performs camera tracking, human pose estimation and
human-scene reconstruction in an online fashion. 3D Gaussian Splatting is
utilized to learn Gaussian primitives for humans and scenes efficiently, and
reconstruction-based camera tracking and human pose estimation modules are
designed to enable holistic understanding and effective disentanglement of pose
and appearance. Specifically, we design a human deformation module to
reconstruct the details and enhance generalizability to out-of-distribution
poses faithfully. Aiming to learn the spatial correlation between human and
scene accurately, we introduce occlusion-aware human silhouette rendering and
monocular geometric priors, which further improve reconstruction quality.
Experiments on the EMDB and NeuMan datasets demonstrate superior or on-par
performance with existing methods in camera tracking, human pose estimation,
novel view synthesis and runtime. Our project page is at
&lt;a href=&#34;https://eth-ait.github.io/ODHSR&#34;&gt;https://eth-ait.github.io/ODHSR&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>PCBEAR: Pose Concept Bottleneck for Explainable Action Recognition</title>
      <link>//localhost:1313/paper/pcbear-pose-concept-bottleneck-for-explainable-action-recognition-1744971733104-279072/</link>
      <pubDate>Fri, 18 Apr 2025 10:22:13 +0000</pubDate>
      <guid>//localhost:1313/paper/pcbear-pose-concept-bottleneck-for-explainable-action-recognition-1744971733104-279072/</guid>
      <description>&lt;p&gt;Human action recognition (HAR) has achieved impressive results with deep
learning models, but their decision-making process remains opaque due to their
black-box nature. Ensuring interpretability is crucial, especially for
real-world applications requiring transparency and accountability. Existing
video XAI methods primarily rely on feature attribution or static textual
concepts, both of which struggle to capture motion dynamics and temporal
dependencies essential for action understanding. To address these challenges,
we propose Pose Concept Bottleneck for Explainable Action Recognition (PCBEAR),
a novel concept bottleneck framework that introduces human pose sequences as
motion-aware, structured concepts for video action recognition. Unlike methods
based on pixel-level features or static textual descriptions, PCBEAR leverages
human skeleton poses, which focus solely on body movements, providing robust
and interpretable explanations of motion dynamics. We define two types of
pose-based concepts: static pose concepts for spatial configurations at
individual frames, and dynamic pose concepts for motion patterns across
multiple frames. To construct these concepts, PCBEAR applies clustering to
video pose sequences, allowing for automatic discovery of meaningful concepts
without manual annotation. We validate PCBEAR on KTH, Penn-Action, and HAA500,
showing that it achieves high classification performance while offering
interpretable, motion-driven explanations. Our method provides both strong
predictive performance and human-understandable insights into the model&amp;rsquo;s
reasoning process, enabling test-time interventions for debugging and improving
model behavior.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Perception Encoder: The best visual embeddings are not at the output of the network</title>
      <link>//localhost:1313/paper/perception-encoder-the-best-visual-embeddings-are-not-at-the-output-of-the-network-1744971733103-444486/</link>
      <pubDate>Fri, 18 Apr 2025 10:22:13 +0000</pubDate>
      <guid>//localhost:1313/paper/perception-encoder-the-best-visual-embeddings-are-not-at-the-output-of-the-network-1744971733103-444486/</guid>
      <description>&lt;p&gt;We introduce Perception Encoder (PE), a state-of-the-art encoder for image
and video understanding trained via simple vision-language learning.
Traditionally, vision encoders have relied on a variety of pretraining
objectives, each tailored to specific downstream tasks such as classification,
captioning, or localization. Surprisingly, after scaling our carefully tuned
image pretraining recipe and refining with our robust video data engine, we
find that contrastive vision-language training alone can produce strong,
general embeddings for all of these downstream tasks. There is only one caveat:
these embeddings are hidden within the intermediate layers of the network. To
draw them out, we introduce two alignment methods, language alignment for
multimodal language modeling, and spatial alignment for dense prediction.
Together with the core contrastive checkpoint, our PE family of models achieves
state-of-the-art performance on a wide variety of tasks, including zero-shot
image and video classification and retrieval; document, image, and video Q&amp;amp;A;
and spatial tasks such as detection, depth estimation, and tracking. To foster
further research, we are releasing our models, code, and a novel dataset of
synthetically and human-annotated videos.&lt;/p&gt;</description>
    </item>
    <item>
      <title>PerceptionLM: Open-Access Data and Models for Detailed Visual Understanding</title>
      <link>//localhost:1313/paper/perceptionlm-open-access-data-and-models-for-detailed-visual-understanding-1744971733104-795729/</link>
      <pubDate>Fri, 18 Apr 2025 10:22:13 +0000</pubDate>
      <guid>//localhost:1313/paper/perceptionlm-open-access-data-and-models-for-detailed-visual-understanding-1744971733104-795729/</guid>
      <description>&lt;p&gt;Vision-language models are integral to computer vision research, yet many
high-performing models remain closed-source, obscuring their data, design and
training recipe. The research community has responded by using distillation
from black-box models to label training data, achieving strong benchmark
results, at the cost of measurable scientific progress. However, without
knowing the details of the teacher model and its data sources, scientific
progress remains difficult to measure. In this paper, we study building a
Perception Language Model (PLM) in a fully open and reproducible framework for
transparent research in image and video understanding. We analyze standard
training pipelines without distillation from proprietary models and explore
large-scale synthetic data to identify critical data gaps, particularly in
detailed video understanding. To bridge these gaps, we release 2.8M
human-labeled instances of fine-grained video question-answer pairs and
spatio-temporally grounded video captions. Additionally, we introduce
PLM-VideoBench, a suite for evaluating challenging video understanding tasks
focusing on the ability to reason about &amp;quot;what&amp;quot;, &amp;quot;where&amp;quot;, &amp;quot;when&amp;quot;, and &amp;quot;how&amp;quot; of a
video. We make our work fully reproducible by providing data, training recipes,
code &amp;amp; models.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Quantum algorithm for solving nonlinear differential equations based on physics-informed effective Hamiltonians</title>
      <link>//localhost:1313/paper/quantum-algorithm-for-solving-nonlinear-differential-equations-based-on-physics-informed-effective-hamiltonians-1744971733104-937656/</link>
      <pubDate>Fri, 18 Apr 2025 10:22:13 +0000</pubDate>
      <guid>//localhost:1313/paper/quantum-algorithm-for-solving-nonlinear-differential-equations-based-on-physics-informed-effective-hamiltonians-1744971733104-937656/</guid>
      <description>&lt;p&gt;We propose a distinct approach to solving linear and nonlinear differential
equations (DEs) on quantum computers by encoding the problem into ground states
of effective Hamiltonian operators. Our algorithm relies on constructing such
operators in the Chebyshev space, where an effective Hamiltonian is a sum of
global differential and data constraints. Once the effective Hamiltonian is
formed, solutions of differential equations can be obtained using the ground
state preparation techniques (e.g. imaginary-time evolution and quantum
singular value transformation), bypassing variational search. Unlike approaches
based on discrete grids, the algorithm enables evaluation of solutions beyond
fixed grid points and implements constraints in the physics-informed way. Our
proposal inherits the best traits from quantum machine learning-based DE
solving (compact basis representation, automatic differentiation, nonlinearity)
and quantum linear algebra-based approaches (fine-grid encoding, provable
speed-up for state preparation), offering a robust strategy for quantum
scientific computing in the early fault-tolerant era.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Readable Twins of Unreadable Models</title>
      <link>//localhost:1313/paper/readable-twins-of-unreadable-models-1744971733104-261389/</link>
      <pubDate>Fri, 18 Apr 2025 10:22:13 +0000</pubDate>
      <guid>//localhost:1313/paper/readable-twins-of-unreadable-models-1744971733104-261389/</guid>
      <description>&lt;p&gt;Creating responsible artificial intelligence (AI) systems is an important
issue in contemporary research and development of works on AI. One of the
characteristics of responsible AI systems is their explainability. In the
paper, we are interested in explainable deep learning (XDL) systems. On the
basis of the creation of digital twins of physical objects, we introduce the
idea of creating readable twins (in the form of imprecise information flow
models) for unreadable deep learning models. The complete procedure for
switching from the deep learning model (DLM) to the imprecise information flow
model (IIFM) is presented. The proposed approach is illustrated with an example
of a deep learning classification model for image recognition of handwritten
digits from the MNIST data set.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Role of AI in Predictive Modeling #ai #artificialintelligence #machinelearning #aiagent #Role</title>
      <link>//localhost:1313/video/role-of-ai-in-predictive-modeling-ai-artificialintelligence-machinelearning-aiagent-role-1744971733105-152523/</link>
      <pubDate>Fri, 18 Apr 2025 10:22:13 +0000</pubDate>
      <guid>//localhost:1313/video/role-of-ai-in-predictive-modeling-ai-artificialintelligence-machinelearning-aiagent-role-1744971733105-152523/</guid>
      <description>&lt;p&gt;genaiexp Artificial Intelligence plays a crucial role in predictive modeling by leveraging machine learning algorithms to analyze &amp;hellip;&lt;/p&gt;</description>
    </item>
    <item>
      <title>RUKA: Rethinking the Design of Humanoid Hands with Learning</title>
      <link>//localhost:1313/paper/ruka-rethinking-the-design-of-humanoid-hands-with-learning-1744971733104-525498/</link>
      <pubDate>Fri, 18 Apr 2025 10:22:13 +0000</pubDate>
      <guid>//localhost:1313/paper/ruka-rethinking-the-design-of-humanoid-hands-with-learning-1744971733104-525498/</guid>
      <description>&lt;p&gt;Dexterous manipulation is a fundamental capability for robotic systems, yet
progress has been limited by hardware trade-offs between precision,
compactness, strength, and affordability. Existing control methods impose
compromises on hand designs and applications. However, learning-based
approaches present opportunities to rethink these trade-offs, particularly to
address challenges with tendon-driven actuation and low-cost materials. This
work presents RUKA, a tendon-driven humanoid hand that is compact, affordable,
and capable. Made from 3D-printed parts and off-the-shelf components, RUKA has
5 fingers with 15 underactuated degrees of freedom enabling diverse human-like
grasps. Its tendon-driven actuation allows powerful grasping in a compact,
human-sized form factor. To address control challenges, we learn
joint-to-actuator and fingertip-to-actuator models from motion-capture data
collected by the MANUS glove, leveraging the hand&amp;rsquo;s morphological accuracy.
Extensive evaluations demonstrate RUKA&amp;rsquo;s superior reachability, durability, and
strength compared to other robotic hands. Teleoperation tasks further showcase
RUKA&amp;rsquo;s dexterous movements. The open-source design and assembly instructions of
RUKA, code, and data are available at &lt;a href=&#34;https://ruka-hand.github.io/&#34;&gt;https://ruka-hand.github.io/&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Save me!! üò≠üò≠üò≠üò≠ #ai #animalintelligence #artificialintelligence #animals #cuteanimals #bunny</title>
      <link>//localhost:1313/video/save-me-ai-animalintelligence-artificialintelligence-animals-cuteanimals-bunny-1744971733105-965386/</link>
      <pubDate>Fri, 18 Apr 2025 10:22:13 +0000</pubDate>
      <guid>//localhost:1313/video/save-me-ai-animalintelligence-artificialintelligence-animals-cuteanimals-bunny-1744971733105-965386/</guid>
      <description></description>
    </item>
    <item>
      <title>Sleep-time Compute: Beyond Inference Scaling at Test-time</title>
      <link>//localhost:1313/paper/sleep-time-compute-beyond-inference-scaling-at-test-time-1744971733104-277155/</link>
      <pubDate>Fri, 18 Apr 2025 10:22:13 +0000</pubDate>
      <guid>//localhost:1313/paper/sleep-time-compute-beyond-inference-scaling-at-test-time-1744971733104-277155/</guid>
      <description>&lt;p&gt;Scaling test-time compute has emerged as a key ingredient for enabling large
language models (LLMs) to solve difficult problems, but comes with high latency
and inference cost. We introduce sleep-time compute, which allows models to
&amp;quot;think&amp;quot; offline about contexts before queries are presented: by anticipating
what queries users might ask and pre-computing useful quantities, we can
significantly reduce the compute requirements at test-time. To demonstrate the
efficacy of our method, we create modified versions of two reasoning tasks -
Stateful GSM-Symbolic and Stateful AIME. We find that sleep-time compute can
reduce the amount of test-time compute needed to achieve the same accuracy by ~
5x on Stateful GSM-Symbolic and Stateful AIME and that by scaling sleep-time
compute we can further increase accuracy by up to 13% on Stateful GSM-Symbolic
and 18% on Stateful AIME. Furthermore, we introduce Multi-Query GSM-Symbolic,
which extends GSM-Symbolic by including multiple related queries per context.
By amortizing sleep-time compute across related queries about the same context
using Multi-Query GSM-Symbolic, we can decrease the average cost per query by
2.5x. We then conduct additional analysis to understand when sleep-time compute
is most effective, finding the predictability of the user query to be well
correlated with the efficacy of sleep-time compute. Finally, we conduct a
case-study of applying sleep-time compute to a realistic agentic SWE task.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Syntactic and Semantic Control of Large Language Models via Sequential Monte Carlo</title>
      <link>//localhost:1313/paper/syntactic-and-semantic-control-of-large-language-models-via-sequential-monte-carlo-1744971733104-674525/</link>
      <pubDate>Fri, 18 Apr 2025 10:22:13 +0000</pubDate>
      <guid>//localhost:1313/paper/syntactic-and-semantic-control-of-large-language-models-via-sequential-monte-carlo-1744971733104-674525/</guid>
      <description>&lt;p&gt;A wide range of LM applications require generating text that conforms to
syntactic or semantic constraints. Imposing such constraints can be naturally
framed as probabilistic conditioning, but exact generation from the resulting
distribution &amp;ndash; which can differ substantially from the LM&amp;rsquo;s base distribution
&amp;ndash; is generally intractable. In this work, we develop an architecture for
controlled LM generation based on sequential Monte Carlo (SMC). Our SMC
framework allows us to flexibly incorporate domain- and problem-specific
constraints at inference time, and efficiently reallocate computational
resources in light of new information during the course of generation. By
comparing to a number of alternatives and ablations on four challenging domains
&amp;ndash; Python code generation for data science, text-to-SQL, goal inference, and
molecule synthesis &amp;ndash; we demonstrate that, with little overhead, our approach
allows small open-source language models to outperform models over 8x larger,
as well as closed-source, fine-tuned ones. In support of the probabilistic
perspective, we show that these performance improvements are driven by better
approximation to the posterior distribution. Our system builds on the framework
of Lew et al. (2023) and integrates with its language model probabilistic
programming language, giving users a simple, programmable way to apply SMC to a
broad variety of controlled generation problems.&lt;/p&gt;</description>
    </item>
    <item>
      <title>The video was created using artificial intelligence. #aiartmodellookbook #aiart #boxgirl boxgi</title>
      <link>//localhost:1313/video/the-video-was-created-using-artificial-intelligence-aiartmodellookbook-aiart-boxgirl-boxgi-1744971733104-810145/</link>
      <pubDate>Fri, 18 Apr 2025 10:22:13 +0000</pubDate>
      <guid>//localhost:1313/video/the-video-was-created-using-artificial-intelligence-aiartmodellookbook-aiart-boxgirl-boxgi-1744971733104-810145/</guid>
      <description></description>
    </item>
    <item>
      <title>This Robot Fixes Itself After Getting Damaged üò≥ #shorts #futuretech #artificialintelligence</title>
      <link>//localhost:1313/video/this-robot-fixes-itself-after-getting-damaged-shorts-futuretech-artificialintelligence-1744971733105-642941/</link>
      <pubDate>Fri, 18 Apr 2025 10:22:13 +0000</pubDate>
      <guid>//localhost:1313/video/this-robot-fixes-itself-after-getting-damaged-shorts-futuretech-artificialintelligence-1744971733105-642941/</guid>
      <description></description>
    </item>
    <item>
      <title>Top Artificial Intelligence and Data Science Course in Chennai</title>
      <link>//localhost:1313/video/top-artificial-intelligence-and-data-science-course-in-chennai-1744971733105-61972/</link>
      <pubDate>Fri, 18 Apr 2025 10:22:13 +0000</pubDate>
      <guid>//localhost:1313/video/top-artificial-intelligence-and-data-science-course-in-chennai-1744971733105-61972/</guid>
      <description>&lt;p&gt;AI Made Easy! Dreaming of a career in Artificial Intelligence or Data Science? Now you can learn cutting-edge AI tech affordably &amp;hellip;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Transfer Learning via Auxiliary Labels with Application to Cold-Hardiness Prediction</title>
      <link>//localhost:1313/paper/transfer-learning-via-auxiliary-labels-with-application-to-cold-hardiness-prediction-1744971733104-667233/</link>
      <pubDate>Fri, 18 Apr 2025 10:22:13 +0000</pubDate>
      <guid>//localhost:1313/paper/transfer-learning-via-auxiliary-labels-with-application-to-cold-hardiness-prediction-1744971733104-667233/</guid>
      <description>&lt;p&gt;Cold temperatures can cause significant frost damage to fruit crops depending
on their resilience, or cold hardiness, which changes throughout the dormancy
season. This has led to the development of predictive cold-hardiness models,
which help farmers decide when to deploy expensive frost-mitigation measures.
Unfortunately, cold-hardiness data for model training is only available for
some fruit cultivars due to the need for specialized equipment and expertise.
Rather, farmers often do have years of phenological data (e.g. date of
budbreak) that they regularly collect for their crops. In this work, we
introduce a new transfer-learning framework, Transfer via Auxiliary Labels
(TAL), that allows farmers to leverage the phenological data to produce more
accurate cold-hardiness predictions, even when no cold-hardiness data is
available for their specific crop. The framework assumes a set of source tasks
(cultivars) where each has associated primary labels (cold hardiness) and
auxiliary labels (phenology). However, the target task (new cultivar) is
assumed to only have the auxiliary labels. The goal of TAL is to predict
primary labels for the target task via transfer from the source tasks.
Surprisingly, despite the vast literature on transfer learning, to our
knowledge, the TAL formulation has not been previously addressed. Thus, we
propose several new TAL approaches based on model selection and averaging that
can leverage recent deep multi-task models for cold-hardiness prediction. Our
results on real-world cold-hardiness and phenological data for multiple grape
cultivars demonstrate that TAL can leverage the phenological data to improve
cold-hardiness predictions in the absence of cold-hardiness data.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Two Harvards undergrad rigged smart Glasses With artificial intelligence to identify complete strang</title>
      <link>//localhost:1313/video/two-harvards-undergrad-rigged-smart-glasses-with-artificial-intelligence-to-identify-complete-strang-1744971733105-105041/</link>
      <pubDate>Fri, 18 Apr 2025 10:22:13 +0000</pubDate>
      <guid>//localhost:1313/video/two-harvards-undergrad-rigged-smart-glasses-with-artificial-intelligence-to-identify-complete-strang-1744971733105-105041/</guid>
      <description></description>
    </item>
    <item>
      <title>video with artificial intelligence #hulk #artificialintelligence #shorts #viralshorts #lion</title>
      <link>//localhost:1313/video/video-with-artificial-intelligence-hulk-artificialintelligence-shorts-viralshorts-lion-1744971733105-567813/</link>
      <pubDate>Fri, 18 Apr 2025 10:22:13 +0000</pubDate>
      <guid>//localhost:1313/video/video-with-artificial-intelligence-hulk-artificialintelligence-shorts-viralshorts-lion-1744971733105-567813/</guid>
      <description></description>
    </item>
    <item>
      <title>ViTa-Zero: Zero-shot Visuotactile Object 6D Pose Estimation</title>
      <link>//localhost:1313/paper/vita-zero-zero-shot-visuotactile-object-6d-pose-estimation-1744971733103-65724/</link>
      <pubDate>Fri, 18 Apr 2025 10:22:13 +0000</pubDate>
      <guid>//localhost:1313/paper/vita-zero-zero-shot-visuotactile-object-6d-pose-estimation-1744971733103-65724/</guid>
      <description>&lt;p&gt;Object 6D pose estimation is a critical challenge in robotics, particularly
for manipulation tasks. While prior research combining visual and tactile
(visuotactile) information has shown promise, these approaches often struggle
with generalization due to the limited availability of visuotactile data. In
this paper, we introduce ViTa-Zero, a zero-shot visuotactile pose estimation
framework. Our key innovation lies in leveraging a visual model as its backbone
and performing feasibility checking and test-time optimization based on
physical constraints derived from tactile and proprioceptive observations.
Specifically, we model the gripper-object interaction as a spring-mass system,
where tactile sensors induce attractive forces, and proprioception generates
repulsive forces. We validate our framework through experiments on a real-world
robot setup, demonstrating its effectiveness across representative visual
backbones and manipulation scenarios, including grasping, object picking, and
bimanual handover. Compared to the visual models, our approach overcomes some
drastic failure modes while tracking the in-hand object pose. In our
experiments, our approach shows an average increase of 55% in AUC of ADD-S and
60% in ADD, along with an 80% lower position error compared to FoundationPose.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
